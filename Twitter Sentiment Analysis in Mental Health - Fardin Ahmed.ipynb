{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6efaeffe",
   "metadata": {},
   "source": [
    "# Human Centered Computing in Mental Health:\n",
    "# Twitter Sentiment Analysis\n",
    "## By: Fardin Ahmed\n",
    "## CS 4395 - Senior Project\n",
    "## ======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91fe16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Human Centered Computing in Mental Health:\n",
      "Twitter Sentiment Analysis\n",
      "\n",
      "Created by: Fardin Ahmed\n",
      "CS 4395 - Senior Project\n",
      "University of Houston Downtown - 2023\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: Please install and upgrade all libraries provided before running this program. Thank you.\n",
    "\n",
    "# =====================================================================================================================\n",
    "# Importing Libraries\n",
    "#=====================================================================================================================\n",
    "\n",
    "import emoji\n",
    "import glob\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from IPython.display import clear_output\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import STOPWORDS\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "#=====================================================================================================================\n",
    "# Global variables - Directory\n",
    "#=====================================================================================================================\n",
    "\n",
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Global variables - Directory\n",
    "folder_path = './Twitter Data'\n",
    "target_folder = './Twitter Data/Filtered Tweets/'\n",
    "filtered_folder = './Twitter Data/Filtered Tweets'\n",
    "output_folder = './Twitter Data/Available Datasets/'\n",
    "combined_folder = './Twitter Data/Combined/'\n",
    "output_directory = \"./Twitter Data/Categorized_dataset\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "create_directory(folder_path)\n",
    "create_directory(target_folder)\n",
    "create_directory(filtered_folder)\n",
    "create_directory(output_folder)\n",
    "create_directory(combined_folder)\n",
    "create_directory(output_directory)\n",
    "\n",
    "\n",
    "\n",
    "#=====================================================================================================================\n",
    "# Depressive categories and defining\n",
    "#=====================================================================================================================\n",
    "\n",
    "def define_categories(): \n",
    "    # Emotional State and Struggles\n",
    "    emotional_state_phrases = [\n",
    "        \"struggling\", \"feeling worthless\", \"despair\", \"emotional pain\", \"numbness\", \"deep despair\",\n",
    "        \"unbearable sadness\", \"constant emptiness\", \"uncontrollable emotions\", \"emotional exhaustion\",\n",
    "        \"emotional struggle\", \"endless sadness\", \"endless emptiness\", \"living in darkness\", \"inner demons\",\n",
    "        \"emotional numbness\", \"feeling overwhelmed\", \"coping with depression\", \"life feels meaningless\",\n",
    "        \"dark thoughts\", \"mental anguish\"\n",
    "    ]\n",
    "\n",
    "    # Self-Esteem and Self-Image\n",
    "    self_esteem_phrases = [\n",
    "        \"self-worth\", \"self-loathing\", \"self-hatred\", \"constant self-doubt\", \"self-destructive\",\n",
    "        \"empty inside\", \"worthlessness\", \"meaningless\", \"feel like a failure\", \"no purpose\"\n",
    "    ]\n",
    "\n",
    "    # Isolation and Withdrawal\n",
    "    isolation_phrases = [\n",
    "        \"self-isolation\", \"feeling like giving up\", \"feeling trapped\", \"rock bottom\", \"asking for help\",\n",
    "        \"seeking isolation\", \"social withdrawal\"\n",
    "    ]\n",
    "\n",
    "    # Suicidal Thoughts and Self-Harm\n",
    "    suicidal_self_harm_phrases = [\n",
    "        \"suicidal thoughts\", \"self-harm\", \"end it all\", \"kill myself\", \"end my life\", \"suicide attempt\", \"hurt myself\", \n",
    "        \"want to die\", \"ending my life\", \"cutting myself\", \"killing myself\"\n",
    "    ]\n",
    "\n",
    "    # Treatment and Coping\n",
    "    treatment_phrases = [\n",
    "        \"therapy\", \"therapy progress\", \"therapy appointment\", \"medication adjustment\", \"treatment options\",\n",
    "        \"reaching out for help\"\n",
    "    ]\n",
    "\n",
    "    # Physical and Behavioral Symptoms\n",
    "    physical_behavioral_phrases = [\n",
    "        \"crying\", \"isolated\", \"can't get out of bed\", \"sleep problems\", \"fatigue\", \"weight changes\",\n",
    "        \"insomnia\", \"loss of appetite\", \"constant fatigue\"\n",
    "    ]\n",
    "\n",
    "    # Relationships and Social Stigma\n",
    "    relationships_stigma_phrases = [\n",
    "        \"feeling judged\", \"support system\", \"feeling disconnected\", \"hiding my pain\", \"seeking solace\",\n",
    "        \"constant battle with my mind\", \"feeling judged\"\n",
    "    ]\n",
    "\n",
    "    # Negative Thoughts\n",
    "    negative_thoughts_phrases = [\n",
    "        \"persistent negative thoughts\", \"mind racing\", \"overthinking\", \"mind in chaos\", \"can't concentrate\",\n",
    "        \"I'm so tired\", \"low self-esteem\", \"difficulty concentrating\", \"loss of enjoyment\", \"irritability\"\n",
    "    ]\n",
    "\n",
    "    # Hopelessness and Desperation\n",
    "    hopelessness_phrases = [\n",
    "        \"feeling like a burden\", \"mental health struggles\", \"hopeless\", \"lost all hope\"\n",
    "    ]\n",
    "\n",
    "    # Mental Health Awareness and Seeking Help\n",
    "    mental_health_support_phrases = [\n",
    "        \"mental health awareness\", \"seeking help\", \"medication\", \"treatment options\", \"reaching out for help\",\n",
    "        \"therapy session\", \"stigma\", \"mental illness\", \"supportive friends\"\n",
    "    ]\n",
    "    \n",
    "    categories = {\n",
    "        \"Emotional State and Struggles\": emotional_state_phrases,\n",
    "        \"Self-Esteem and Self-Image\": self_esteem_phrases,\n",
    "        \"Isolation and Withdrawal\": isolation_phrases,\n",
    "        \"Suicidal Thoughts and Self-Harm\": suicidal_self_harm_phrases,\n",
    "        \"Treatment and Coping\": treatment_phrases,\n",
    "        \"Physical and Behavioral Symptoms\": physical_behavioral_phrases,\n",
    "        \"Relationships and Social Stigma\": relationships_stigma_phrases,\n",
    "        \"Negative Thoughts\": negative_thoughts_phrases,\n",
    "        \"Hopelessness and Desperation\": hopelessness_phrases,\n",
    "        \"Mental Health Awareness and Seeking Help\": mental_health_support_phrases\n",
    "    }\n",
    "    return categories\n",
    "\n",
    "\n",
    "\n",
    "#=====================================================================================================================\n",
    "# Preprocessing\n",
    "#=====================================================================================================================\n",
    "\n",
    "# Custom pronouns, function words, and simple verbs as additional stopwords\n",
    "custom_pronouns = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "                   'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "                   'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "                   'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "                   'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', \n",
    "                   'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', \n",
    "                   'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', \n",
    "                   'over', 'under', 'again', 'further', 'then', 'once', 'one', 'dont', 'time', 'ive', 'much',\n",
    "                   'want', 'like', 'get', 'would', 'make', 'take', 'see', 'say', 'think', 'come', 'go', 'know', 'tell', \n",
    "                   'ask', 'work', 'seem', 'try', 'call', 'need', 'use', 'find', 'give', 'show', 'hear', 'play', 'run', \n",
    "                   'move', 'help', 'start', 'stop', 'write', 'be', 'become', 'begin', 'feel', 'bring', 'buy', 'put']\n",
    "\n",
    "\n",
    "# Merge the lists and remove duplicates\n",
    "combined_stopwords = list(set(custom_pronouns))\n",
    "\n",
    "# Initialize the Porter Stemmer, WordNet Lemmatizer\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Set of stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Text cleaning function with stemming, lemmatization, and removal of stopwords\n",
    "def clean_text(text):\n",
    "    if combined_stopwords is not None:\n",
    "        stopwords.update(combined_stopwords)\n",
    "    \n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "\n",
    "    # Tokenize into words using nltk's word_tokenize\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords, apply stemming, and lemmatization\n",
    "    processed_words = [lemmatizer.lemmatize(porter.stem(word)) for word in words if word.lower() not in stopwords]\n",
    "\n",
    "    # Join the processed words back into cleaned text\n",
    "    cleaned_text = ' '.join(processed_words)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "#=====================================================================================================================\n",
    "# Additional Preprocessing\n",
    "#=====================================================================================================================\n",
    "\n",
    "def define_depressive_stop_words():\n",
    "    depressive_stop_words = [\n",
    "        \"adjustment\", \"all\", \"anguish\", \"appetite\", \"appointment\", \"asking\", \"attempt\", \"awareness\", \"battle\", \n",
    "        \"bed\", \"bottom\", \"burden\", \"can\", \"changes\", \"chaos\", \"concentrate\", \"concentrating\", \"constant\", \n",
    "        \"coping\", \"cutting\", \"dark\", \"darkness\", \"deep\", \"demons\", \"depression\", \"destructive\", \"die\", \"difficulty\", \n",
    "        \"disconnected\", \"doubt\", \"emotional\", \"emotions\", \"emptiness\", \"empty\", \"end\", \"ending\", \"endless\", \n",
    "        \"enjoyment\", \"esteem\", \"exhaustion\", \"failure\", \"feel\", \"feeling\", \"feels\", \"for\", \"friends\", \"get\", \n",
    "        \"giving\", \"harm\", \"hatred\", \"health\", \"help\", \"hiding\", \"hope\", \"hurt\", \"illness\", \"in\", \"inner\", \"inside\", \n",
    "        \"isolation\", \"it\", \"judged\", \"kill\", \"killing\", \"life\", \"like\", \"living\", \"loathing\", \"loss\", \"lost\", \"low\", \n",
    "        \"mental\", \"mind\", \"my\", \"myself\", \"negative\", \"no\", \"of\", \"options\", \"out\", \"overwhelmed\", \"pain\", \"persistent\", \n",
    "        \"problems\", \"progress\", \"purpose\", \"racing\", \"reaching\", \"rock\", \"sadness\", \"seeking\", \"self\", \"session\", \"sleep\", \n",
    "        \"so\", \"social\", \"solace\", \"struggle\", \"struggles\", \"suicidal\", \"suicide\", \"support\", \"supportive\", \"system\", \"thoughts\", \n",
    "        \"tired\", \"to\", \"trapped\", \"treatment\", \"unbearable\", \"uncontrollable\", \"up\", \"want\", \"weight\", \"with\", \"withdrawal\", \n",
    "        \"worth\", \"worthless\",\n",
    "        \n",
    "        \"n't\", \"'m\", \"'s\"\n",
    "    ]\n",
    "    return depressive_stop_words\n",
    "\n",
    "\n",
    "\n",
    "#=====================================================================================================================\n",
    "# Title Screen\n",
    "#=====================================================================================================================\n",
    "\n",
    "def display_title_screen():\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Human Centered Computing in Mental Health:\")\n",
    "    print(\"Twitter Sentiment Analysis\\n\")\n",
    "    print(\"Created by: Fardin Ahmed\")\n",
    "    print(\"CS 4395 - Senior Project\")\n",
    "    print(\"University of Houston Downtown - 2023\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    input(\"Press Enter to continue...\")\n",
    "    \n",
    "\n",
    "    \n",
    "#=====================================================================================================================\n",
    "# Listing, checking, and comparing available datasets (Raw and filtered)\n",
    "#=====================================================================================================================\n",
    "\n",
    "def list_available_datasets(folder_path):\n",
    "    # Get a list of dataset files in the folder with a .csv extension\n",
    "    dataset_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "    # Create a folder to store the Notepad file\n",
    "    output_folder = './Twitter Data/Available Datasets/'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Write the available dataset names to a Notepad file in the output folder\n",
    "    notepad_filename = 'available_datasets.txt'\n",
    "    notepad_filepath = os.path.join(output_folder, notepad_filename)\n",
    "\n",
    "    # Prepare the list of available datasets\n",
    "    available_datasets = []\n",
    "\n",
    "    for dataset_file in dataset_files:\n",
    "        dataset_name = os.path.splitext(os.path.basename(dataset_file))[0]\n",
    "        available_datasets.append(dataset_name)\n",
    "\n",
    "    # Write the available dataset names to the Notepad file\n",
    "    with open(notepad_filepath, 'w') as notepad_file:\n",
    "        notepad_file.write('\\n'.join(available_datasets))\n",
    "\n",
    "    # Return the list of available datasets (if needed)\n",
    "    return available_datasets\n",
    "\n",
    "def list_available_filtered_datasets(filtered_folder, output_folder):\n",
    "    # Get a list of dataset files in the folder\n",
    "    dataset_files = glob.glob(os.path.join(filtered_folder, '*.csv'))\n",
    "\n",
    "    # Create a folder to store the Notepad file\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Write the available dataset names to a Notepad file in the output folder\n",
    "    notepad_filename = 'filtered_tweets_records.txt'\n",
    "    notepad_filepath = os.path.join(output_folder, notepad_filename)\n",
    "    with open(notepad_filepath, 'w') as notepad_file:\n",
    "        available_datasets = []\n",
    "        for i, dataset_file in enumerate(dataset_files, start=1):\n",
    "            dataset_name = os.path.splitext(os.path.basename(dataset_file))[0]\n",
    "            available_datasets.append(dataset_name)\n",
    "        notepad_file.write('\\n'.join(available_datasets))\n",
    "        \n",
    "def check_missing_dataset_names(file1_path, file2_path):\n",
    "    try:\n",
    "        # Read the dataset names from both text files\n",
    "        with open(file1_path, 'r') as file1:\n",
    "            available_datasets = set(line.strip() for line in file1)\n",
    "\n",
    "        with open(file2_path, 'r') as file2:\n",
    "            # Remove \"_filtered\" from the dataset names in file2\n",
    "            filtered_datasets = set(line.strip()[:-9] for line in file2)\n",
    "\n",
    "        # Find dataset names in file1 that are not in file2\n",
    "        missing_datasets = available_datasets - filtered_datasets\n",
    "\n",
    "        return missing_datasets\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: One or both of the required text files not found.\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return set()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "#=====================================================================================================================\n",
    "# Loading and Selecting the dataset with analysis on which csv column contains tweets\n",
    "#=====================================================================================================================\n",
    "            \n",
    "def load_dataset(dataset_path):\n",
    "    # Debug print to verify dataset path\n",
    "    print(f\"Loading dataset from: {dataset_path}\")\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path, encoding='utf-8')\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {dataset_path}\")\n",
    "        return None\n",
    "\n",
    "def select_dataset(folder_path):\n",
    "    # List available datasets in the folder\n",
    "    available_datasets = list_available_datasets(folder_path)\n",
    "\n",
    "    # Check if there are any available datasets\n",
    "    if not available_datasets:\n",
    "        print(\"No datasets found in the folder.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Display the list of available datasets\n",
    "    print(\"Available datasets in the folder:\")\n",
    "    for i, dataset in enumerate(available_datasets, start=1):\n",
    "        print(f\"{i}. {dataset}\")\n",
    "\n",
    "    while True:\n",
    "        # Get the user's choice\n",
    "        dataset_choice = input(\"Enter the number of the dataset you want to select: \")\n",
    "\n",
    "        try:\n",
    "            dataset_choice = int(dataset_choice)\n",
    "            if 1 <= dataset_choice <= len(available_datasets):\n",
    "                dataset_name = available_datasets[dataset_choice - 1]\n",
    "                dataset_path = os.path.join(folder_path, f\"{dataset_name}.csv\")\n",
    "                clear_output()\n",
    "\n",
    "                # Debug print to verify dataset path\n",
    "                print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "                # Load the selected dataset into a DataFrame\n",
    "                df = load_dataset(dataset_path)\n",
    "\n",
    "                # Detect the likely tweet column\n",
    "                likely_tweet_column = detect_likely_tweet_column(df)\n",
    "\n",
    "                if likely_tweet_column:\n",
    "                    print(f\"Likely tweet column: {likely_tweet_column}\")\n",
    "                    print(\"Dataset loaded successfully.\")\n",
    "                    print(\"-\" * 50)\n",
    "                    return df, dataset_name, likely_tweet_column\n",
    "                else:\n",
    "                    print(\"No likely tweet column detected in the dataset.\")\n",
    "                    return None, None, None\n",
    "\n",
    "            else:\n",
    "                print(\"Invalid choice. Please enter a valid number.\")\n",
    "                continue\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid number.\")\n",
    "            continue\n",
    "    \n",
    "def detect_likely_tweet_column(df):\n",
    "    # Create a dictionary to store the criteria for each likely tweet column\n",
    "    likely_tweet_columns = {}\n",
    "\n",
    "    # Iterate through the likely tweet columns\n",
    "    for column in df.columns:\n",
    "        print(f\"Analyzing column: {column}\")\n",
    "        if not column.startswith(\"https://\"):\n",
    "            # Calculate the average length of text\n",
    "            average_length = df[column].apply(lambda x: len(str(x))).mean()\n",
    "            \n",
    "            # Calculate the percentage of non-empty values\n",
    "            non_empty_percentage = (df[column].apply(lambda x: isinstance(x, str) and len(str(x).strip()) > 0).sum() / len(df)) * 100\n",
    "            \n",
    "            # Calculate the percentage of unique values\n",
    "            unique_percentage = (len(set(df[column])) / len(df)) * 100\n",
    "            \n",
    "            likely_tweet_columns[column] = {\n",
    "                'average_length': average_length,\n",
    "                'non_empty_percentage': non_empty_percentage,\n",
    "                'unique_percentage': unique_percentage\n",
    "            }\n",
    "\n",
    "    # Determine the most likely tweet column based on criteria\n",
    "    if likely_tweet_columns:\n",
    "        weights = {\n",
    "            'average_length': 1,\n",
    "            'non_empty_percentage': 2,\n",
    "            'unique_percentage': 3\n",
    "        }\n",
    "\n",
    "        # Calculate a score for each column based on the weighted criteria\n",
    "        for column, criteria in likely_tweet_columns.items():\n",
    "            score = sum(weights[criterion] * criteria[criterion] for criterion in criteria)\n",
    "            likely_tweet_columns[column]['score'] = score\n",
    "\n",
    "        # Find the column with the highest score\n",
    "        most_likely_tweet_column = max(likely_tweet_columns, key=lambda x: likely_tweet_columns[x]['score'])\n",
    "\n",
    "        # Return the most likely tweet column\n",
    "        return most_likely_tweet_column\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#=====================================================================================================================\n",
    "# Option panel to see raw tweets or filtered tweets with saving to a filtered csv file and combining\n",
    "#=====================================================================================================================\n",
    "\n",
    "def choose_operation(df, likely_tweet_column, dataset_name, target_folder, categories):\n",
    "    filtered_tweets_saved = False\n",
    "    \n",
    "    while True:\n",
    "        # Get the user's choice\n",
    "        choice = input(\"What would you like to do?\\n1. See all tweets\\n2. See only filtered tweets\\n3. Return to main menu\\n\")\n",
    "\n",
    "        if choice == '1':\n",
    "            clear_output()\n",
    "            print(\"I'm fetching all the tweets. This could take a while\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # Pass the likely_tweet_column to print_rows_with_column function\n",
    "            print_rows_with_column(df, likely_tweet_column)\n",
    "\n",
    "        elif choice == '2':\n",
    "            clear_output()\n",
    "            print(\"Analyzing and displaying filtered tweets...\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Apply the classification and sentiment analysis function to each tweet in the specified column\n",
    "            df[['Tweet', 'Sentiment', 'Classified Categories']] = df[likely_tweet_column].apply(lambda x: classify_and_analyze(x, categories)).apply(pd.Series)\n",
    "\n",
    "            # Filter the DataFrame to include only rows with classified categories\n",
    "            filtered_df = df[df['Classified Categories'].apply(lambda x: len(x) > 0) | df['Classified Categories'].isnull()]\n",
    "\n",
    "            # Sort the filtered DataFrame by sentiment score in ascending order\n",
    "            filtered_df = filtered_df.sort_values(by='Sentiment', ascending=True)\n",
    "\n",
    "            # Display the results\n",
    "            for index, row in filtered_df.iterrows():\n",
    "                print(f\"Tweet {index + 1}:\")\n",
    "                print(row['Tweet'])\n",
    "                print()\n",
    "                print(f\"Sentiment Score: {row['Sentiment']:.2f}\")\n",
    "                print(\"Classified Categories:\", ', '.join(row['Classified Categories']))\n",
    "                print(\"-\" * 50)\n",
    "            \n",
    "            save_filtered_tweets_to_csv(filtered_df, dataset_name, target_folder)\n",
    "            combine_and_save_filtered_datasets(filtered_folder, combined_folder)\n",
    "\n",
    "        elif choice == '3':\n",
    "            clear_output()\n",
    "            return\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a valid number (1, 2, or 3).\")\n",
    "\n",
    "            \n",
    "            \n",
    "#=====================================================================================================================\n",
    "# Printing rows and columns from either individual datasets or the combined dataset\n",
    "#=====================================================================================================================\n",
    "            \n",
    "def print_rows_with_column(df, column):\n",
    "    if column in df.columns:\n",
    "        # Iterate through the DataFrame and print individual rows with the specified column\n",
    "        for index, row in df.iterrows():\n",
    "            if column in row:\n",
    "                long_text = row[column]\n",
    "                print(f\"Row {index + 1}: \")\n",
    "                print(long_text)\n",
    "                print(\"-\" * 50)\n",
    "        if not any(column in row for _, row in df.iterrows()):\n",
    "            print(\"No rows found for the specified column.\")\n",
    "    else:\n",
    "        print(f\"The '{column}' column is not found in the DataFrame.\")    \n",
    "\n",
    "\n",
    "        \n",
    "#=====================================================================================================================\n",
    "# Data Processing\n",
    "#=====================================================================================================================\n",
    "        \n",
    "def classify_and_analyze(tweet, categories):\n",
    "    tweet = str(tweet).lower()  # Ensure tweet is a string and convert to lowercase\n",
    "    sentiment = TextBlob(tweet).sentiment.polarity\n",
    "    classifications = []\n",
    "\n",
    "    # Classify the tweet into categories\n",
    "    for category, phrases in categories.items():\n",
    "        if any(phrase.lower() in tweet for phrase in phrases):\n",
    "            classifications.append(category)\n",
    "\n",
    "    return tweet, sentiment, classifications \n",
    "\n",
    "def save_filtered_tweets_to_csv(filtered_df, dataset_name, target_folder):\n",
    "    # Specify the path and filename for the CSV file to save the results\n",
    "    csv_filename = f'{dataset_name}_filtered.csv'\n",
    "\n",
    "    # Check if the CSV file already exists\n",
    "    try:\n",
    "        existing_df = pd.read_csv(csv_filename)\n",
    "\n",
    "        # Combine the existing data with the new data and remove duplicates based on the 'Tweet' column\n",
    "        combined_df = pd.concat([existing_df, filtered_df[['Tweet', 'Classified Categories', 'Sentiment']]], ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset='Tweet', inplace=True)\n",
    "\n",
    "        # Save the combined DataFrame back to the CSV file\n",
    "        combined_df.to_csv(csv_filename, columns=['Tweet', 'Classified Categories', 'Sentiment'], index=False)\n",
    "\n",
    "        # Move the CSV file to the target folder\n",
    "        shutil.move(csv_filename, os.path.join(target_folder, csv_filename))\n",
    "\n",
    "        print(f\"Filtered tweets appended to '{csv_filename}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, simply save the filtered data to a new CSV file\n",
    "        filtered_df[['Tweet', 'Classified Categories', 'Sentiment']].to_csv(csv_filename, index=False)\n",
    "\n",
    "        # Move the CSV file to the target folder\n",
    "        shutil.move(csv_filename, os.path.join(target_folder, csv_filename))\n",
    "\n",
    "        print(f\"Filtered tweets saved to '{csv_filename}'\")\n",
    "\n",
    "def combine_and_save_filtered_datasets(filtered_folder, combined_folder):\n",
    "    # Get a list of all the _filtered.csv files in the folder\n",
    "    filtered_files = glob.glob(os.path.join(filtered_folder, '*_filtered.csv'))\n",
    "\n",
    "    # Initialize an empty DataFrame to store the combined data\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "    # Loop through each _filtered.csv file and append it to the combined_data DataFrame\n",
    "    for file in filtered_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            pass  # Skip empty files if any\n",
    "\n",
    "    # Remove duplicates based on the 'Tweet' column\n",
    "    combined_data.drop_duplicates(subset='Tweet', inplace=True)\n",
    "\n",
    "    # Specify the target folder to save the combined dataset\n",
    "    os.makedirs(combined_folder, exist_ok=True)\n",
    "\n",
    "    # Specify the filename for the combined dataset\n",
    "    combined_filename = 'combined_filtered_tweets.csv'\n",
    "\n",
    "    # Create the full path for the combined dataset\n",
    "    combined_filepath = os.path.join(combined_folder, combined_filename)\n",
    "\n",
    "    # Save the combined data to the specified location\n",
    "    combined_data.to_csv(combined_filepath, columns=['Tweet', 'Classified Categories', 'Sentiment'], index=False)\n",
    "\n",
    "    print(f\"Now combined to '{combined_filepath}'\")\n",
    "\n",
    "def filter_and_combine_datasets(folder_path, filtered_folder, combined_folder, categories):\n",
    "    # Get a list of all the CSV files in the dataset folder\n",
    "    dataset_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "    # Initialize an empty DataFrame to store the combined data\n",
    "    combined_data = pd.DataFrame()\n",
    "\n",
    "  \n",
    "    # Loop through each CSV file and process it\n",
    "    for dataset_file in dataset_files:\n",
    "        dataset_name = os.path.splitext(os.path.basename(dataset_file))[0]\n",
    "        filtered_filename = os.path.join(filtered_folder, f\"{dataset_name}_filtered.csv\")\n",
    "\n",
    "        # Check if the filtered dataset already exists\n",
    "        if os.path.exists(filtered_filename):\n",
    "            print(f\"Filtered dataset for '{dataset_name}' already exists. Skipping filtering.\")\n",
    "        else:\n",
    "            print(f\"Filtering dataset '{dataset_name}'...\")\n",
    "            try:\n",
    "                df = pd.read_csv(dataset_file)\n",
    "\n",
    "                # Detect the likely tweet column\n",
    "                likely_tweet_column = detect_likely_tweet_column(df)\n",
    "\n",
    "                if likely_tweet_column:\n",
    "                    # Proceed with the existing code using the likely tweet column\n",
    "                    df[['Tweet', 'Sentiment', 'Classified Categories']] = df[likely_tweet_column].apply(lambda x: classify_and_analyze(x, categories)).apply(pd.Series)\n",
    "\n",
    "                    # Filter the DataFrame to include only rows with classified categories\n",
    "                    filtered_df = df[df['Classified Categories'].apply(lambda x: len(x) > 0) | df['Classified Categories'].isnull()]\n",
    "\n",
    "                    # Save the filtered data to its own CSV file\n",
    "                    filtered_df[['Tweet', 'Classified Categories', 'Sentiment']].to_csv(filtered_filename, index=False)\n",
    "\n",
    "                    print(f\"Filtered dataset for '{dataset_name}' saved to '{filtered_filename}'\")\n",
    "                else:\n",
    "                    print(f\"No likely tweet column detected in the dataset. Skipping '{dataset_name}'\")\n",
    "\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Skipping empty dataset: '{dataset_name}'\")\n",
    "\n",
    "        # Load the filtered dataset and append it to the combined_data DataFrame\n",
    "        try:\n",
    "            filtered_df = pd.read_csv(filtered_filename)\n",
    "            combined_data = pd.concat([combined_data, filtered_df], ignore_index=True)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    # Remove duplicates based on the 'Tweet' column\n",
    "    combined_data.drop_duplicates(subset='Tweet', inplace=True)\n",
    "\n",
    "    # Specify the target folder to save the combined dataset\n",
    "    os.makedirs(combined_folder, exist_ok=True)\n",
    "\n",
    "    # Specify the filename for the combined dataset\n",
    "    combined_filename = 'combined_filtered_tweets.csv'\n",
    "\n",
    "    # Create the full path for the combined dataset\n",
    "    combined_filepath = os.path.join(combined_folder, combined_filename)\n",
    "\n",
    "    # Save the combined data to the specified location\n",
    "    combined_data.to_csv(combined_filepath, columns=['Tweet', 'Classified Categories', 'Sentiment'], index=False)\n",
    "\n",
    "    print(f\"Combined datasets saved to '{combined_filepath}'\")\n",
    "    \n",
    "#=====================================================================================================================\n",
    "# Individual Dataset Analysis\n",
    "#=====================================================================================================================\n",
    "    \n",
    "def select_dataset_from_folder(folder_path):\n",
    "    # List all files in the folder\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    if not files:\n",
    "        print(f\"No datasets found in the folder: {folder_path}\")\n",
    "        return None\n",
    "\n",
    "    # Display the available datasets\n",
    "    print(\"Available datasets:\")\n",
    "    for i, file in enumerate(files, start=1):\n",
    "        print(f\"{i}. {file}\")\n",
    "\n",
    "    # Get user input for the dataset choice\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter the number of the dataset you want to select: \"))\n",
    "            if 1 <= choice <= len(files):\n",
    "                selected_dataset = files[choice - 1]\n",
    "                clear_output()\n",
    "                print(f\"You have selected: {selected_dataset}\")\n",
    "                return selected_dataset\n",
    "            else:\n",
    "                print(\"Invalid choice. Please enter a valid number.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "    \n",
    "def analyze_individual_dataset(filtered_folder):\n",
    "    filtered_folder = './Twitter Data/Filtered Tweets'\n",
    "    selected_dataset = select_dataset_from_folder(filtered_folder)\n",
    "    clear_output()\n",
    "\n",
    "    if selected_dataset:\n",
    "        dataset_path = os.path.join(filtered_folder, selected_dataset)\n",
    "\n",
    "        # Generate a full report for the selected dataset\n",
    "        report_name = f\"Report_{os.path.splitext(os.path.basename(selected_dataset))[0]}\"\n",
    "        report_path = os.path.join(output_folder, report_name)\n",
    "        generate_report(dataset_path, report_path)\n",
    "        print()\n",
    "\n",
    "    else:\n",
    "        print(\"No dataset selected. Exiting analysis.\")\n",
    "    \n",
    "#=====================================================================================================================\n",
    "# Combined Data Split and Data Analysis\n",
    "#=====================================================================================================================\n",
    "    \n",
    "def split_combined_dataset(combined_folder, output_dir):\n",
    "    # Load the combined dataset\n",
    "    df = pd.read_csv(combined_folder)\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize a dictionary to store datasets by category\n",
    "    category_datasets = {}\n",
    "\n",
    "    # Iterate through the rows of the combined dataset\n",
    "    for index, row in df.iterrows():\n",
    "        categories = row['Classified Categories']\n",
    "        tweet_text = row['Tweet']\n",
    "        sentiment_score = row['Sentiment']\n",
    "\n",
    "        # Split the multiple categories\n",
    "        for category in categories.split(', '):\n",
    "            # Stripping any '[' and ']' characters from the category name\n",
    "            category = category.strip('[]')\n",
    "\n",
    "            # Create a new DataFrame for the current category\n",
    "            category_df = pd.DataFrame({'Tweet': [tweet_text], 'Classified Categories': [category], 'Sentiment': [sentiment_score]})\n",
    "\n",
    "            # Define the path to save the dataset for the current category\n",
    "            category_output_path = os.path.join(output_dir, f\"{category}_dataset.csv\")\n",
    "\n",
    "            # If the file already exists in the dictionary, append to it\n",
    "            if category_output_path in category_datasets:\n",
    "                category_datasets[category_output_path] = pd.concat([category_datasets[category_output_path], category_df], ignore_index=True)\n",
    "            else:\n",
    "                category_datasets[category_output_path] = category_df\n",
    "\n",
    "    # Save each dataset to the corresponding file\n",
    "    for category_output_path, category_df in category_datasets.items():\n",
    "        category_df.to_csv(category_output_path, index=False)\n",
    "    \n",
    "def analyze_combined_dataset(combined_folder):\n",
    "    # Manually assigning the folder to bypass permission issues\n",
    "    combined_folder = \"./Twitter Data/Combined/combined_filtered_tweets.csv\"\n",
    "    \n",
    "    # Splitting the combined Datasets into Categorized Dataset\n",
    "    print(\"Updating dataset. Please wait a moment.\")\n",
    "    split_combined_dataset(combined_folder, output_directory)\n",
    "    clear_output()\n",
    "\n",
    "    # Load the combined dataset\n",
    "    combined_df = pd.read_csv(combined_folder)\n",
    "\n",
    "    while True:\n",
    "        print(\"What would you like to do?\")\n",
    "        print(\"1. Analyze the combined dataset\")\n",
    "        print(\"2. Analyze a specific categorized dataset\")\n",
    "        print(\"3. Exit to main menu\")\n",
    "        choice = input(\"Enter the number of your choice: \")\n",
    "\n",
    "        if choice == \"1\":\n",
    "            clear_output()\n",
    "            \n",
    "            # Function to analyze the combined dataset\n",
    "            analyze_combined(combined_folder)\n",
    "        elif choice == \"2\":\n",
    "            clear_output()\n",
    "            categorized_datasets = [file for file in os.listdir(\"./Twitter Data/Categorized_dataset\") if file.endswith('.csv')]\n",
    "            print(\"Available categorized datasets:\")\n",
    "            for i, dataset in enumerate(categorized_datasets, 1):\n",
    "                print(f\"{i}. {dataset}\")\n",
    "\n",
    "            dataset_choice = int(input(\"Enter the number of the dataset you want to analyze: \")) - 1\n",
    "\n",
    "            if 0 <= dataset_choice < len(categorized_datasets):\n",
    "                dataset_path = os.path.join(\"./Twitter Data/Categorized_dataset\", categorized_datasets[dataset_choice])\n",
    "                categorized_df = pd.read_csv(dataset_path)\n",
    "                clear_output()\n",
    "                print(f\"Analyzing dataset: {categorized_datasets[dataset_choice]}\")\n",
    "                print()\n",
    "\n",
    "                while True:\n",
    "                    print(\"What would you like to do with the selected dataset?\")\n",
    "                    print(\"1. Read all tweets\")\n",
    "                    print(\"2. Generate Report\")                    \n",
    "                    print(\"3. Go back to the main menu\")\n",
    "                    analysis_option = input(\"Enter the number of your choice: \")\n",
    "\n",
    "                    if analysis_option == \"1\":\n",
    "                        clear_output()\n",
    "                        read_all_tweets(dataset_path)\n",
    "                    elif analysis_option == \"2\":\n",
    "                        clear_output()\n",
    "                        generate_report(dataset_path, dataset.replace('.csv', ''))\n",
    "                    elif analysis_option == \"3\":\n",
    "                        clear_output()\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Invalid choice. Please enter a valid number (1, 2, or 3).\")\n",
    "            else:\n",
    "                print(\"Invalid choice.\")\n",
    "        elif choice == \"3\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a valid number (1, 2, or 3).\")\n",
    "\n",
    "def analyze_combined(combined_folder):\n",
    "    combined_folder = \"./Twitter Data/Combined/combined_filtered_tweets.csv\"\n",
    "\n",
    "    # Load the combined dataset\n",
    "    combined_df = pd.read_csv(combined_folder)\n",
    "\n",
    "    while True:\n",
    "        print(\"What would you like to do?\")\n",
    "        print(\"1. Read all overall tweets\")\n",
    "        print(\"2. Generate Report for both combined and separate datasets\")\n",
    "        print(\"3. Go back to the main menu\")\n",
    "        choice = input(\"Enter the number of your choice: \")\n",
    "\n",
    "        if choice == \"1\":\n",
    "            clear_output()\n",
    "            \n",
    "            # Read all tweets from the combined dataset\n",
    "            read_all_tweets(combined_folder)\n",
    "        elif choice == \"2\":\n",
    "            clear_output()\n",
    "            \n",
    "            # Generate the full report\n",
    "            generate_report(combined_folder, \"Combined Dataset\")\n",
    "            categorized_datasets = [file for file in os.listdir(\"./Twitter Data/Categorized_dataset\") if file.endswith('.csv')]\n",
    "            for dataset in categorized_datasets:\n",
    "                dataset_path = os.path.join(\"./Twitter Data/Categorized_dataset\", dataset)\n",
    "                generate_report(dataset_path, dataset.replace('.csv', ''))\n",
    "        elif choice == \"3\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a valid number (1, 2, or 3).\")\n",
    "\n",
    "        \n",
    "\n",
    "#=====================================================================================================================\n",
    "# Visualization of all tweets\n",
    "#=====================================================================================================================           \n",
    "\n",
    "# Function to read all tweets in a dataset\n",
    "def read_all_tweets(dataset_path):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        tweet = row['Tweet']\n",
    "        sentiment_score = row['Sentiment']\n",
    "\n",
    "        print(f\"Row {index + 1}: \")\n",
    "        print(f\"Tweet: {tweet}\")\n",
    "        print(f\"Sentiment Score: {sentiment_score}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "#=====================================================================================================================\n",
    "# Visualization using Wordcloud and Bar Chart\n",
    "#=====================================================================================================================         \n",
    "\n",
    "def display_wordclouds(df, dataset_name):\n",
    "    # Separate positive and negative tweets based on sentiment score\n",
    "    positive_tweets = df[df['Sentiment'] > 0]\n",
    "    negative_tweets = df[df['Sentiment'] < 0]\n",
    "\n",
    "    # All cleaned positive and negative tweets into single strings\n",
    "    positive_tweets_text = ' '.join(positive_tweets['Cleaned_Text'].tolist())\n",
    "    negative_tweets_text = ' '.join(negative_tweets['Cleaned_Text'].tolist())\n",
    "\n",
    "    # Getting NLTK English stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # Generate the word clouds for positive and negative tweets\n",
    "    positive_wordcloud = WordCloud(width=400, height=200, background_color='white', stopwords=stopwords).generate(\n",
    "        positive_tweets_text)\n",
    "    negative_wordcloud = WordCloud(width=400, height=200, background_color='white', stopwords=stopwords).generate(\n",
    "        negative_tweets_text)\n",
    "\n",
    "    # Display word clouds side by side\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Positive Word Cloud from {dataset_name} Tweets')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(negative_wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Negative Word Cloud from {dataset_name} Tweets')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def display_top_words_bar_chart(df, dataset_name):\n",
    "    # Separate positive and negative tweets based on binary sentiment labels\n",
    "    positive_tweets = df[df['Sentiment'] == 1]\n",
    "    negative_tweets = df[df['Sentiment'] == 0]\n",
    "\n",
    "    # Concatenate all cleaned positive and negative tweets into single strings\n",
    "    positive_tweets_text = ' '.join(positive_tweets['Cleaned_Text'].tolist())\n",
    "    negative_tweets_text = ' '.join(negative_tweets['Cleaned_Text'].tolist())\n",
    "\n",
    "    # Remove stopwords and custom pronouns from positive and negative tweets\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    positive_words = [word for word in nltk.word_tokenize(positive_tweets_text) if word.lower() not in stopwords]\n",
    "\n",
    "    # If there are no positive words, there will be neutral words\n",
    "    if not positive_words:\n",
    "        all_words = nltk.word_tokenize(' '.join(df['Cleaned_Text'].tolist()))\n",
    "        neutral_words = [word for word in all_words if word.lower() not in stopwords]\n",
    "        positive_words = neutral_words\n",
    "\n",
    "    # Create frequency distribution for positive (or neutral) tweets\n",
    "    positive_freq_dist = nltk.FreqDist(positive_words)\n",
    "    top_positive_words = sorted(positive_freq_dist.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    positive_words, positive_counts = zip(*top_positive_words)\n",
    "\n",
    "    # Remove stopwords and custom pronouns from negative tweets\n",
    "    negative_words = [word for word in nltk.word_tokenize(negative_tweets_text) if word.lower() not in stopwords]\n",
    "\n",
    "    # If there are no negative words, there will be neutral words\n",
    "    if not negative_words: \n",
    "        negative_words = neutral_words\n",
    "\n",
    "    # Create frequency distribution for negative tweets\n",
    "    negative_freq_dist = nltk.FreqDist(negative_words)\n",
    "    top_negative_words = sorted(negative_freq_dist.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    negative_words, negative_counts = zip(*top_negative_words)\n",
    "\n",
    "    # Display the top 20 words in horizontal bar graphs for positive and negative tweets\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    axes[0].barh(positive_words, positive_counts, color='green')\n",
    "    axes[0].set_title(f'Top 20 Words in Positive {dataset_name}')\n",
    "    axes[0].set_xlabel('Frequency')\n",
    "    axes[0].set_ylabel('Words')\n",
    "\n",
    "    axes[1].barh(negative_words, negative_counts, color='red')\n",
    "    axes[1].set_title(f'Top 20 Words in Negative {dataset_name}')\n",
    "    axes[1].set_xlabel('Frequency')\n",
    "    axes[1].set_ylabel('Words')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#=====================================================================================================================\n",
    "# Generate Report\n",
    "#=====================================================================================================================        \n",
    "\n",
    "def generate_report(dataset_path, dataset_name, custom_pronouns=None, sentiment_filter=None, num_topics=5):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Apply text cleaning function\n",
    "    df['Cleaned_Text'] = df['Tweet'].apply(clean_text)\n",
    "\n",
    "    # Apply sentiment filte\n",
    "    if sentiment_filter is not None:\n",
    "        df = filter_sentiment(df, sentiment_filter)\n",
    "\n",
    "    # Display word clouds for positive and negative tweets\n",
    "    display_wordclouds(df, dataset_name)\n",
    "\n",
    "    # Display top 20 words in horizontal bar graphs for positive and negative tweets\n",
    "    display_top_words_bar_chart(df, dataset_name)\n",
    "\n",
    "    # Train and evaluate machine learning models\n",
    "    train_and_evaluate_models(df)\n",
    "\n",
    "    # Perform topic modeling\n",
    "    perform_topic_modeling(df, num_topics)\n",
    "    print()\n",
    "    \n",
    "    # Removing depressive words\n",
    "    categories = define_categories()\n",
    "\n",
    "    # Combine all words from different categories into a single list\n",
    "    all_depressive_words = [word for category_words in categories.values() for word in category_words]\n",
    "\n",
    "    # Converting the words to lowercase to ensure case-insensitivity\n",
    "    all_depressive_words = [word.lower() for word in all_depressive_words]\n",
    "\n",
    "    # Perform topic modeling with depressive words as stopwords\n",
    "    perform_topic_modeling_trends(df, num_topics, depressive_words=all_depressive_words)\n",
    "\n",
    "#=====================================================================================================================\n",
    "# Machine Learning\n",
    "#=====================================================================================================================\n",
    "\n",
    "def train_and_evaluate_models(df):\n",
    "    X = df['Cleaned_Text']\n",
    "    y = df['Sentiment']\n",
    "\n",
    "    # Convert sentiment scores to binary labels (0 or 1)\n",
    "    y_binary = (y > 0).astype(int)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Vectorize the text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train and evaluate a Naive Bayes classifier\n",
    "    clf_nb = MultinomialNB()\n",
    "    evaluate_model(clf_nb, X_train_tfidf, X_test_tfidf, y_train, y_test, \"Naive Bayes\")\n",
    "\n",
    "    # Train and evaluate a Support Vector Machine (SVM) classifier\n",
    "    clf_svm = SVC()\n",
    "    evaluate_model(clf_svm, X_train_tfidf, X_test_tfidf, y_train, y_test, \"SVM\")\n",
    "\n",
    "    # Train and evaluate a Logistic Regression classifier\n",
    "    clf_lr = LogisticRegression()\n",
    "    evaluate_model(clf_lr, X_train_tfidf, X_test_tfidf, y_train, y_test, \"Logistic Regression\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    unique_classes = np.unique(y_train)\n",
    "    if len(unique_classes) < 2:\n",
    "        print(f\"Only one class present in the training set. Unable to train {model_name}.\")\n",
    "        return    \n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "        print(f\"{model_name} Precision: {precision:.2f}\")\n",
    "        print(f\"{model_name} Recall: {recall:.2f}\")\n",
    "        print(f\"{model_name} F1-Score: {fscore:.2f}\")\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        print(f\"{model_name} Confusion Matrix:\")\n",
    "        print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n",
    "\n",
    "        # Calculate true positive rate (TPR), false positive rate (FPR), true negative rate (TNR), and false negative rate (FNR)\n",
    "        tpr = tp / (tp + fn)\n",
    "        fpr = fp / (fp + tn)\n",
    "        tnr = tn / (tn + fp)\n",
    "        fnr = fn / (fn + tp)\n",
    "\n",
    "        print(f\"{model_name} True Positive Rate (TPR): {tpr:.2f}\")\n",
    "        print(f\"{model_name} False Positive Rate (FPR): {fpr:.2f}\")\n",
    "        print(f\"{model_name} True Negative Rate (TNR): {tnr:.2f}\")\n",
    "        print(f\"{model_name} False Negative Rate (FNR): {fnr:.2f}\")\n",
    "    else:\n",
    "        print(f\"Only one class present in the test set. Unable to calculate precision, recall, and confusion matrix.\")\n",
    "        print(f\"Consider uploading more datasets. It can be found on kaggle.com\")\n",
    "    print()\n",
    "\n",
    "#=====================================================================================================================\n",
    "#  Topic Modeling\n",
    "#=====================================================================================================================    \n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic #{topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))   \n",
    "\n",
    "def perform_topic_modeling(df, num_topics):\n",
    "    texts = [word_tokenize(text) for text in df['Cleaned_Text']]\n",
    "    \n",
    "    # Vectorize the text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = vectorizer.fit_transform(df['Cleaned_Text'])\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_model.fit(X_tfidf)\n",
    "\n",
    "    # Print the topics\n",
    "    print(\"Topics from Latent Dirichlet Allocation (LDA):\")\n",
    "    if hasattr(vectorizer, 'get_feature_names_out'):\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "    else:\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "    display_topics(lda_model, feature_names, 10)  \n",
    "\n",
    "def perform_topic_modeling_trends(df, num_topics, depressive_words=None):\n",
    "    # Tokenize the text data\n",
    "    texts = [word_tokenize(text) for text in df['Cleaned_Text']]\n",
    "\n",
    "    depressive_stop_words = define_depressive_stop_words()\n",
    "\n",
    "    # Define the vectorizer with optional depressive words as stopwords\n",
    "    vectorizer = TfidfVectorizer(stop_words=depressive_stop_words)\n",
    "    \n",
    "    # Vectorize the text data using TF-IDF\n",
    "    X_tfidf = vectorizer.fit_transform(df['Cleaned_Text'])\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_model.fit(X_tfidf)\n",
    "\n",
    "    # Print the topics\n",
    "    print(\"Detailed topics and sources from Latent Dirichlet Allocation (LDA):\")\n",
    "    if hasattr(vectorizer, 'get_feature_names_out'):\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "    else:\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "    display_topics(lda_model, feature_names, 10)\n",
    "    \n",
    "#=================================================================================================================\n",
    "#=================================================================================================================\n",
    "# Main function\n",
    "#=================================================================================================================\n",
    "#=================================================================================================================\n",
    "\n",
    "def main():\n",
    "    clear_output()\n",
    "\n",
    "    # Display the title screen\n",
    "    display_title_screen()\n",
    "    clear_output()\n",
    "    \n",
    "    # Check if the 'Twitter Data' folder exists, and create it if not. Crucial to new users\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Created folder: {folder_path}\")    \n",
    "    \n",
    "    # Check if there are any datasets in the specified folder\n",
    "    dataset_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "    if not dataset_files:\n",
    "        print(\"No datasets found in the specified folder. \\nPlease download and place datasets in the correct location.\")\n",
    "        print(f\"The expected directory for datasets is: {folder_path}\")\n",
    "        print()\n",
    "        print(\"You can download datasets from Kaggle: https://www.kaggle.com/\")\n",
    "        return\n",
    "\n",
    "    # This will update the filtered dataset into the 'filtered_tweets_records.txt' file for comparison\n",
    "    list_available_datasets(folder_path)\n",
    "    list_available_filtered_datasets(filtered_folder, output_folder)\n",
    "\n",
    "    # Define file paths\n",
    "    file1_path = os.path.join(output_folder, 'available_datasets.txt')\n",
    "    file2_path = os.path.join(output_folder, 'filtered_tweets_records.txt')\n",
    "\n",
    "    # Check for missing datasets\n",
    "    missing_datasets = check_missing_dataset_names(file1_path, file2_path)\n",
    "\n",
    "    # Get the categories\n",
    "    categories = define_categories()\n",
    "\n",
    "    if missing_datasets:\n",
    "        # Print the missing dataset names\n",
    "        print(\"It seems you have these Twitter datasets which aren't yet analyzed and combined in the system:\")\n",
    "        print(\"---------------------------------------------------------------------------------------------\")\n",
    "        for dataset in missing_datasets:\n",
    "            print(dataset)\n",
    "        print()\n",
    "        while True:\n",
    "            print(\"What would you like to do?\")\n",
    "            print(\"1. Filter and Combine the datasets\")\n",
    "            print(\"2. Analyze individual datasets\")\n",
    "            print(\"3. Analyze individual reports (All datasets must be filtered and combined first)\")\n",
    "            print(\"4. Analyze the combined dataset\")\n",
    "            print(\"5. Exit\")\n",
    "\n",
    "            user_choice = input(\"Enter the number of your choice: \")\n",
    "\n",
    "            if user_choice == '1':\n",
    "                clear_output()\n",
    "                print(\"Let me combine all unused datasets. It may take some time.\")\n",
    "                filter_and_combine_datasets(folder_path, filtered_folder, combined_folder, categories)\n",
    "            elif user_choice == '2':\n",
    "                clear_output()\n",
    "                print(\"Okay, let's take a look at the Twitter datasets you have so far\")\n",
    "                print(\"-\" * 50)\n",
    "                df, dataset_name, likely_tweet_column = select_dataset(folder_path)\n",
    "                if df is not None:\n",
    "                    choose_operation(df, likely_tweet_column, dataset_name, target_folder, categories)\n",
    "            elif user_choice == '3':\n",
    "                clear_output()\n",
    "                print(\"We need to filter and combine all datasets first. Give me a moment.\")\n",
    "                filter_and_combine_datasets(folder_path, filtered_folder, combined_folder, categories)\n",
    "                analyze_individual_dataset(filtered_folder)\n",
    "                \n",
    "            elif user_choice == '4':\n",
    "                clear_output()\n",
    "                analyze_combined_dataset(combined_folder)\n",
    "            \n",
    "            elif user_choice == '5':\n",
    "                print(\"Exiting the program.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid choice. Please enter a valid number (1, 2, 3, or 4).\")\n",
    "\n",
    "    else:\n",
    "        print(\"It looks like you have all your Twitter datasets analyzed and combined.\")\n",
    "        print(\"----------------------------------------------------------------------\\n\")\n",
    "        while True:\n",
    "            print(\"What would you like to do?\")\n",
    "            print(\"1. Analyze individual datasets\")\n",
    "            print(\"2. Analyze individual reports\")\n",
    "            print(\"3. Analyze the combined dataset\")\n",
    "            print(\"4. Exit\")\n",
    "\n",
    "            user_choice = input(\"Enter the number of your choice: \")\n",
    "\n",
    "            if user_choice == '1':\n",
    "                clear_output()\n",
    "                print(\"Okay, let's take a look at the Twitter datasets you have so far\")\n",
    "                print(\"-\" * 50)\n",
    "                df, dataset_name, likely_tweet_column = select_dataset(folder_path)\n",
    "                if df is not None:\n",
    "                    choose_operation(df, likely_tweet_column, dataset_name, target_folder, categories)\n",
    "            elif user_choice == '2':\n",
    "                clear_output()\n",
    "                analyze_individual_dataset(filtered_folder)       \n",
    "            \n",
    "            elif user_choice == '3':\n",
    "                clear_output()\n",
    "                analyze_combined_dataset(combined_folder)\n",
    "            elif user_choice == '4':\n",
    "                print(\"Exiting the program.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid choice. Please enter a valid number (1, 2, or 3).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ce2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
